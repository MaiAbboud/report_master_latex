\begin{thebibliography}{ widest-label }
	\selectlanguage{english}
	\bibitem{Zhang21}
	Zhang, X.-Q., Jiang, R.-H., Fan, C.-X., Tong, T.-Y., Wang, T., Huang, P.-C., 2021. Advances in Deep Learning Methods for Visual Tracking: Literature Review and Fundamentals. International Journal of Automation and Computing 18, 311–333. https://doi.org/10.1007/s11633-020-1274-8
	
	\bibitem{Abbass21}
	Abbass, M.Y., Kwon, K.-C., Kim, N., Abdelwahab, S.A., El-Samie, F.E.A., Khalaf, A.A.M., 2021. A survey on online learning for visual tracking. Vis Comput 37, 993–1014. https://doi.org/10.1007/s00371-020-01848-y
	
	\bibitem{TrTr}
	Zhao, M., Okada, K., Inaba, M., 2021. TrTr: Visual Tracking with Transformer. arXiv:2105.03817 [cs].
	
	\bibitem{Marvasti}
	Marvasti-Zadeh, S.M., Cheng, L., Ghanei-Yakhdan, H., Kasaei, S., 2021. Deep Learning for Visual Tracking: A Comprehensive Survey. IEEE Transactions on Intelligent Transportation Systems 1–26. https://doi.org/10.1109/TITS.2020.3046478
	
	\bibitem{MDNet}
	Nam, H., Han, B., 2016. Learning Multi-Domain Convolutional Neural Networks for Visual Tracking. https://doi.org/10.48550/arXiv.1510.07945
	
	\bibitem{GOTURN}
	Held, D., Thrun, S., Savarese, S., 2016. Learning to Track at 100 FPS with Deep Regression Networks. arXiv:1604.01802 [cs].
	
	\bibitem{Ma18}
	D. Ma, W. Bu, and X. Wu, “Multi-scale recurrent tracking via pyramid recurrent network and optical flow,” in Proc. BMVC, 2018, p. 242.
	
	\bibitem{Vaswani17}
	Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I., 2017. Attention Is All You Need. https://doi.org/10.48550/arXiv.1706.03762
	
	\bibitem{DETR}
	Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S., 2020. End-to-End Object Detection with Transformers. arXiv:2005.12872 [cs].
	
	\bibitem{swinTrack}
	Lin, L., Fan, H., Xu, Y., Ling, H., 2021. SwinTrack: A Simple and Strong Baseline for Transformer Tracking. https://doi.org/10.48550/arXiv.2112.00995
	
	\bibitem{Stark}
	Yan, B., Peng, H., Fu, J., Wang, D., Lu, H., 2021. Learning Spatio-Temporal Transformer for Visual Tracking. arXiv:2103.17154 [cs].
	
	\bibitem{ResNet}
	He, K., Zhang, X., Ren, S. and Sun, J., 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778).
	
	\bibitem{VGG}
	Chatfield, K., Simonyan, K., Vedaldi, A. and Zisserman, A., 2014. Return of the devil in the details: Delving deep into convolutional nets. arXiv preprint arXiv:1405.3531.
	
	\bibitem{got10k}
	Huang, L., Zhao, X., Huang, K., 2021. GOT-10k: A Large High-Diversity Benchmark for Generic Object Tracking in the Wild. IEEE Trans. Pattern Anal. Mach. Intell. 43, 1562–1577. https://doi.org/10.1109/TPAMI.2019.2957464
	
	
	\bibitem{Lasot}
	Chen, Y.H., Wang, C.Y., Yang, C.Y., Chang, H.S., Lin, Y.L., Chuang, Y.Y. and Liao, H.Y.M., 2022. NeighborTrack: Improving Single Object Tracking by Bipartite Matching with Neighbor Tracklets. arXiv preprint arXiv:2211.06663.
	
	\bibitem{Trackingnet}
	Muller, M., Bibi, A., Giancola, S., Alsubaihi, S. and Ghanem, B., 2018. Trackingnet: A large-scale dataset and benchmark for object tracking in the wild. In Proceedings of the European conference on computer vision (ECCV) (pp. 300-317).
	
	\bibitem{SiamRPN++}
	B. Li, W. Wu, Q. Wang, F. Zhang, J. Xing, and J. Yan, “SiamRPN++: Evolution of Siamese visual tracking with very deep networks,” 2018. [Online]. Available: http://arxiv.org/abs/1812.11703
	
	\bibitem{DeepTrack}
	H. Li and et al., “DeepTrack: Learning discriminative feature representations online for robust visual tracking,” IEEE Trans. Image Process., vol. 25, no. 4, pp. 1834–1848, 2016.
	
	\bibitem{SMART}
	J. Gao, T. Zhang, and C. Xu, “SMART: Joint sampling and regression for visual tracking,” IEEE Trans. Image Process., vol. 28, no. 8, pp. 3923–3935, 2019.
	
	\bibitem{ATOM}
	Danelljan, M., Bhat, G., Khan, F.S., Felsberg, M., 2019. ATOM: Accurate Tracking by Overlap Maximization. arXiv:1811.07628 [cs].
	
	\bibitem{DIMP}
	G. Bhat, M. Danelljan, L. V. Gool, and R. Timofte, “Learning discriminative model prediction for tracking,” 2019. [Online]. Available: http://arxiv.org/abs/1904.07220
	
	
	\bibitem{prDIMP}
	Danelljan, M., Van Gool, L., Timofte, R., 2020. Probabilistic Regression for Visual Tracking.
	
	\bibitem{FocalLoss}
	Lin, T.-Y., Goyal, P., Girshick, R., He, K., Dollár, P., 2017. Focal Loss for Dense Object Detection. https://doi.org/10.48550/arXiv.1708.02002
	
	\bibitem{varifocal}
	Zhang, H., Wang, Y., Dayoub, F. and Sunderhauf, N., 2021. Varifocalnet: An iou-aware dense object detector. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 8514-8523).
	
	\bibitem{SiamFC}
	Bertinetto, L., Valmadre, J., Henriques, J.F., Vedaldi, A., Torr, P.H.S., 2021. Fully-Convolutional Siamese Networks for Object Tracking. https://doi.org/10.48550/arXiv.1606.09549
	
	\bibitem{SiamAtt}
	Y. Yu, Y. Xiong, W. Huang, and M. R. Scott, “Deformable siamese attention networks for visual object tracking,” in Proc. IEEE CVPR, 2020.
	
	\bibitem{Mot15}
	Ji, Qingge , Yu, Haoqiang , Wu, Xiao. (2020). Hierarchical-Matching-Based Online and Real-Time Multi-Object Tracking with Deep Appearance Features. Algorithms. 13. 80. 10.3390/a13040080. 
	
	\bibitem{swintransformer}
	Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B., 2021. Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. https://doi.org/10.48550/arXiv.2103.14030
	
	\bibitem{giou}
	Rezatofighi, H., Tsoi, N., Gwak, J., Sadeghian, A., Reid, I., Savarese, S., 2019. Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression. https://doi.org/10.48550/arXiv.1902.09630
	
	\bibitem{transformertracker}
	Chen, X., Yan, B., Zhu, J., Wang, D., Yang, X., Lu, H., 2021. Transformer Tracking. arXiv:2103.15436 [cs].
	
	\bibitem{alexnet}
	Krizhevsky, A., Sutskever, I. and Hinton, G.E., 2017. Imagenet classification with deep convolutional neural networks. Communications of the ACM, 60(6), pp.84-90.
	
	\bibitem{untiedPE}
	Ke, G., He, D., Liu, T.-Y., 2021. Rethinking Positional Encoding in Language Pre-training. https://doi.org/10.48550/arXiv.2006.15595
	
	\bibitem{generalfocalloss}
	Li, X., Wang, W., Wu, L., Chen, S., Hu, X., Li, J., Tang, J., Yang, J., 2020. Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection. https://doi.org/10.48550/arXiv.2006.04388
	
	\bibitem{web:visualize}
	visualizing a neural machine translation model article
	
	\bibitem{RNN-gentleIntro}
	Recurrent Neural Networks (RNNs): A gentle Introduction and Overview https://arxiv.org/pdf/1912.05911.pdf
	
	\bibitem{d2l_rnn}
	https://d2l.ai/chapterrecurrent-modern/seq2seq.html
	
	\bibitem{RNN-unfold}
	https://dennybritz.com/posts/wildml/recurrent-neural-networks-tutorial-part-1/
	
	\bibitem{TransformerCV}
	Transformer in computer vision: Farewell convolution 2020
	
	\bibitem{IllustratedAttention}
	https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3
	
	\bibitem{Bahdanau2016}
	Bahdanau, D., Cho, K., Bengio, Y., 2016. Neural Machine Translation by Jointly Learning to Align and Translate. https://doi.org/10.48550/arXiv.1409.0473
	
	\bibitem{Luong2015}
	Luong, M.-T., Pham, H., Manning, C.D., 2015. Effective Approaches to Attention-based Neural Machine Translation. https://doi.org/10.48550/arXiv.1508.04025
	
	\bibitem{illustratedTransformer}
	https://jalammar.github.io/illustrated-transformer/
	
	\bibitem{residual}
	He, K., Zhang, X., Ren, S. and Sun, J., 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778).
	
	\bibitem{LN}
	Ba, J.L., Kiros, J.R., Hinton, G.E., 2016. Layer Normalization. https://doi.org/10.48550/arXiv.1607.06450
	
	\bibitem{web:PE}
	Habshee, J.A., 2020. On Positional Encodings in the Attention Mechanism. Medium. URL https://medium.com/@j.ali.hab/on-positional-encodings-in-the-attention-mechanism-ee81e6076b62 (accessed 10.20.22).
	
	\bibitem{web:IOU_fig}
	Rosebrock, A., 2016. Intersection over Union (IoU) for object detection. PyImageSearch. URL https://pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/ (accessed 12.7.22).
	
	\bibitem{swinTrackWeights}
	SwinTrack - Google Drive [WWW Document], n.d. URL https://drive.google.com/drive/folders/1zPlgAs9D20g04$\_$RWPPgTUg2j0C6A7adJ (accessed 12.7.22).
	
	\bibitem{web:wandb}
	"https://wandb.ai/site"
	
	\bibitem{ViTsurvey}
	Liu, Y., Zhang, Yao, Wang, Y., Hou, F., Yuan, J., Tian, J., Zhang, Yang, Shi, Z., Fan, J., He, Z., 2021. A Survey of Visual Transformers. arXiv:2111.06091 [cs].
	
	\bibitem{Image Transformer}
	Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, Ł., Shazeer, N., Ku, A., Tran, D., 2018. Image Transformer. arXiv:1802.05751 [cs].
	
	\bibitem{Transgan}
	Jiang, Y., Chang, S. and Wang, Z., 2021. Transgan: Two pure transformers can make one strong gan, and that can scale up. Advances in Neural Information Processing Systems, 34, pp.14745-14758.
	
	\bibitem{ViT}
	Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N., 2021. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. arXiv:2010.11929 [cs].
	
	\bibitem{Pre-trainedimageTransformer}
	Chen, H., Wang, Y., Guo, T., Xu, C., Deng, Y., Liu, Z., Ma, S., Xu, C., Xu, C. and Gao, W., 2021. Pre-trained image processing transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 12299-12310).
	
	\bibitem{Non-local neural networks}
	Wang, X., Girshick, R., Gupta, A. and He, K., 2018. Non-local neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 7794-7803).
	
	\bibitem{Swin Transformer V1 and V2}
	Swin Transformer V1 and V2 — Best Vision Models Are Not CNN-based | by Leo Wang | Medium [WWW Document], n.d. URL https://medium.com/@mlquest0/swin-transformer-v1-v2-best-vision-models-are-not-cnn-based-afd51172af31 (accessed 12.8.22).
	
	\bibitem{VT}
	Wu, B., Xu, C., Dai, X., Wan, A., Zhang, P., Yan, Z., Tomizuka, M., Gonzalez, J., Keutzer, K. and Vajda, P., 2020. Visual transformers: Token-based image representation and processing for computer vision. arXiv preprint arXiv:2006.03677.
	
	\bibitem{BotNet}
	Srinivas, A., Lin, T.Y., Parmar, N., Shlens, J., Abbeel, P. and Vaswani, A., 2021. Bottleneck transformers for visual recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 16519-16529).
	
	\bibitem{BEiT}
	Bao, H., Dong, L. and Wei, F., 2021. Beit: Bert pre-training of image transformers. arXiv preprint arXiv:2106.08254.
	
	\bibitem{ConViT}
	d’Ascoli, S., Touvron, H., Leavitt, M.L., Morcos, A.S., Biroli, G. and Sagun, L., 2021, July. Convit: Improving vision transformers with soft convolutional inductive biases. In International Conference on Machine Learning (pp. 2286-2296). PMLR.
	
	\bibitem{TNT}
	Han, K., Xiao, A., Wu, E., Guo, J., Xu, C. and Wang, Y., 2021. Transformer in transformer. Advances in Neural Information Processing Systems, 34, pp.15908-15919.
	
	\bibitem{Volo}
	Yuan, L., Hou, Q., Jiang, Z., Feng, J. and Yan, S., 2022. Volo: Vision outlooker for visual recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence.
	
	\bibitem{T2T-ViT}
	Yuan, L., Chen, Y., Wang, T., Yu, W., Shi, Y., Jiang, Z.H., Tay, F.E., Feng, J. and Yan, S., 2021. Tokens-to-token vit: Training vision transformers from scratch on imagenet. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 558-567).
	
	\bibitem{PVT}
	Wang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P. and Shao, L., 2021. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 568-578).
	
	\bibitem{Why Deep Learning Works}
	P. P. Brahma, D. Wu and Y. She, "Why Deep Learning Works: A Manifold Disentanglement Perspective," in IEEE Transactions on Neural Networks and Learning Systems, vol. 27, no. 10, pp. 1997-2008, Oct. 2016, doi: 10.1109/TNNLS.2015.2496947.
	
	\bibitem{Conditional positional encodings for ViT}
	Chu, X., Tian, Z., Zhang, B., Wang, X., Wei, X., Xia, H. and Shen, C., 2021. Conditional positional encodings for vision transformers. arXiv preprint arXiv:2102.10882.
	
	\bibitem{RethinkingPE}
	Wu, K., Peng, H., Chen, M., Fu, J. and Chao, H., 2021. Rethinking and improving relative position encoding for vision transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 10033-10041).
	
	\bibitem{deeper look at position information in cnns}
	Islam, M.A., Kowal, M., Jia, S., Derpanis, K.G. and Bruce, N.D., 2021. Position, padding and predictions: A deeper look at position information in cnns. arXiv preprint arXiv:2101.12322.
	
	\bibitem{Cordonnier}
	Cordonnier, J.B., Loukas, A. and Jaggi, M., 2019. On the relationship between self-attention and convolutional layers. arXiv preprint arXiv:1911.03584.
	
	\bibitem{Attention is not all you need}
	Dong, Y., Cordonnier, J.B. and Loukas, A., 2021, July. Attention is not all you need: Pure attention loses rank doubly exponentially with depth. In International Conference on Machine Learning (pp. 2793-2803). PMLR.
	
	\bibitem{DeformableDETR}
	Zhu, X., Su, W., Lu, L., Li, B., Wang, X. and Dai, J., 2020. Deformable detr: Deformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159.
	
	\bibitem{ACT}
	Zheng, M., Gao, P., Zhang, R., Li, K., Wang, X., Li, H. and Dong, H., 2020. End-to-end object detection with adaptive clustering transformer. arXiv preprint arXiv:2011.09315.
	
	\bibitem{TSP}
	Sun, Z., Cao, S., Yang, Y. and Kitani, K.M., 2021. Rethinking transformer-based set prediction for object detection. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 3611-3620).
	
	\bibitem{YOLOS}
	Fang, Y., Liao, B., Wang, X., Fang, J., Qi, J., Wu, R., Niu, J. and Liu, W., 2021. You only look at one sequence: Rethinking transformer in vision through object detection. Advances in Neural Information Processing Systems, 34, pp.26183-26197.
	
	\bibitem{FPT}
	Zhang, D., Zhang, H., Tang, J., Wang, M., Hua, X. and Sun, Q., 2020, August. Feature pyramid transformer. In European conference on computer vision (pp. 323-339). Springer, Cham.
	
	\bibitem{BERT}
	Devlin, J., Chang, M.W., Lee, K. and Toutanova, K., 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.
	
	\bibitem{Feature pyramid networks for object detection}
	Lin, T.Y., Dollár, P., Girshick, R., He, K., Hariharan, B. and Belongie, S., 2017. Feature pyramid networks for object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 2117-2125).
	
	\bibitem{An analysis of scale invariance in object detection}
	Singh, B. and Davis, L.S., 2018. An analysis of scale invariance in object detection snip. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 3578-3587).
	
	\bibitem{web:attention in cv}
	Fernandez, J., 2022. Attention in computer vision [WWW Document]. Medium. URL https://towardsdatascience.com/attention-in-computer-vision-fd289a5bd7ad (accessed 10.4.22).
	
	\bibitem{AdamW}
	Loshchilov, I., Hutter, F., 2019. Decoupled Weight Decay Regularization. https://doi.org/10.48550/arXiv.1711.05101
	
	\bibitem{TrackFormer}
	Meinhardt, T., Kirillov, A., Leal-Taixe, L., Feichtenhofer, C., 2022. TrackFormer: Multi-Object Tracking with Transformers.
	
	\bibitem{TransTrack}
	Sun, P., Cao, J., Jiang, Y., Zhang, R., Xie, E., Yuan, Z., Wang, C., Luo, P., 2021. TransTrack: Multiple Object Tracking with Transformer.
	\selectlanguage{arabic}
\end{thebibliography}
