%\chapter{المحول 
%\textLR{Transformer}}
%\section{مقدمة}
%بما أن النموذج الأساسي في البحث ملاحق
%\textLR{SwinTrack\cite{swinTrack}}
%يعتمد بشكل أساسي على المحول من نمط
%\textLR{Swin\cite{swintransformer}}
%وعلى توابع الانتباه المستخدمة في نموذج المحول الأصلي
%\textLR{\cite{Vaswani17}}،
%لذلك وجب شرح نموذج المحول بشكل مفصل.
%\newline
%سنذكر في هذا الفصل النماذج السابقة له، وكيفية تطور بنيته،  سنشرح البنية الكاملة لنموذج المحول، ومن ثم سنشرح التابع الأساسي المستخدم في المحول والمستخدم في البحث وهو تابع الانتباه
%\textLR{attnetion}
%بشكليه الذاتي والتقاطعي.
% وسنستعرض تطبيقات المحول في الرؤية الصنعية, وسنتحدث بالتفصيل عن محول 
%\textLR{Swin}،
%وأخيرا سنذكر تطبيقات المحول في الملاحقة.
%
%\section{النماذج السابقة للمحول}
%صمم نموذج المحول
%\textLR{\cite{Vaswani17}}
%ليكون إحدى نماذج
%\textLR{seq2seq}،
%وهي نماذج يكون كل من دخلها وخرجها عبارة عن سلاسل من العناصر. من الممكن أن تكون السلسة أي نوع من المعلومات ككلمات، صور، محارف أو غيرها
%\textLR{\cite{web:visualize}}.
%\newline
%معظم تطبيقات نماذج
%\textLR{seq2seq}
% كانت في مجال معالجة اللغات الطبيعية
%\textLR{natural language processing NLP}
% أي يكون الدخل عبارة عن سلسلة من كلمات.
%\selectlanguage{english}
%\begin{figure}[!h]
%	\centering
%	\includegraphics[width=\textwidth]{images/RNN}
%	\caption{\cite{d2l_rnn}
%		encoder-decoder
%		\textRL{مع بنية}
%		seq2seq
%		\textRL{نموذج}}
%	\label{fig:RNN1}	
%\end{figure}
%\selectlanguage{arabic}
%\newline
%قبل استخدام نموذج المحول
%كانت البنية الأساسية لنماذج
%\textLR{seq2seq}
%هي الشبكات العصبونية العودية
%\textLR{recurrent neural networks RNN}.
%ولفهم تطور بنية المحول يجب أن نفهم البنية العامة للشبكات العودية وهو ما سنذكره في الفقرة القادمة.
%\newline
%\subsection{الشبكات العصبونية العودية
%\textLR{RNN}
%}
%تُستخدم
%\textLR{RNN}
% بشكل رئيسي للتعرف على الأنماط في المعطيات التسلسلية، كالنصوص أو الفيديو أو أي متسلسلة زمنية. وما يميزها
% عن الشبكات العصبونية التقليدية
%\textLR{MLP Multilayer perceptron}
% هو كيفية مرور المعلومات من خلالها
%\textLR{\cite{RNN-gentleIntro}}.
%
%\selectlanguage{english}
%\begin{figure}[!h]
%	\centering
%	\includegraphics[width=\textwidth]{images/RNN_gentle_introduction}
%	\caption{\cite{RNN-gentleIntro}
%	\textRL{الفرق بين مرور المعلومات بين الشبكات العودية -اليمين، والشبكات التقليدية - اليسار}
%}
%	\label{fig:RNN2}
%\end{figure}
%\selectlanguage{arabic}
%يوضح الشكل
%\ref{fig:RNN1}
% الاختلاف بين
%\textLR{MLP}
% و
%\textLR{RNN}،
% حيث تمرر الشبكات التقليدية المعلومات بدون وجود أي حلقات على العكس من شبكات
%\textLR{RNN}.
%حيث يحسب الخرج في شبكات
%\textLR{MLP}
% كما في المعادلات
%\ref{eq:mlp}
%\begin{equation}
%	\begin{split}
%	& H = \phi_h (XW_{xh} + b_h)\\
%	& O = \phi_o (HW_{ho} + b_o)\\
%	\end{split}
%	\label{eq:mlp}
%\end{equation}
%حيث $H$ خرج الطبقة المخفية، 
%$\phi_h$
%تابع تفعيل الطبقة المخفية،
%$O$
%الخرج النهائي،
%$\phi_o$
%تابع تغعيل طبقة الخرج، 
%$W,b$
%مصفوفات الأوزان وأشعة الانحياز.
%\newline
%أما في 
%\textLR{RNN}
%فمعادلة الخرج في كل خطوة زمنية
%$t$
%تحسب من خلال المعادلات 
%\ref{eq:RNN}
%\begin{equation}
%\begin{split}
%	&H_t = \phi_h(X_t W_{xh} + H_{t-1} W_{hh} + b_h)\\
%	& O_t = \phi_o (H_t W_{ho} + b_o)\\
%\end{split}
%\label{eq:RNN}
%\end{equation}
%حيث
%$H_t$
%يدعى بالحالة المخفية
%\textLR{hidden state}
%في اللحظة
%$t$.
%\newline
%نلاحظ أنه في كل خطوة زمنية لحساب الخرج نحتاج إلى الدخل
%$X_t$
%وإلى الحالة المخفية في الخطوة الزمنية السابقة.
%يوضح الشكل التالي مخارج ومداخل
%\textLR{RNN}
%بعد نشر الشبكة من أجل كل خطوة زمنية.
%\selectlanguage{english}
%\begin{figure}[!h]
%	\centering
%	\includegraphics[width=\textwidth]{images/RNN_unfold}
%	\caption{\cite{RNN-unfold}}
%	\label{fig:RNN3}
%\end{figure}
%\selectlanguage{arabic}
%نلاحظ أيضا أنه لحساب الخرج في لحظة زمنية
%$t$
%نحتاج إلى دخل السلسلة في هذه اللحظة
%$X_t$
%وإلى الحالة المخفية في اللحظة السابقة
%$H_{t-1}$
%أو
%$s_{t-1}$
%كما هو موضح في الشكل
%\ref{fig:RNN3}
%والمعادلات
%\ref{eq:RNN}.
%\newline
%\subsubsection{مشكلات 
%\textLR{RNN}}
%إحدى المشكلات التي تعاني منها شبكات
%\textLR{RNN}
% هي معالجة العناصر بشكل تسلسلي، فكما رأينا سابقا فإن الخرج في كل خطوة زمنية يحتاج إلى معالجة المداخل السابقة بشكل تسلسلي و هذا مايجعل زمن التدريب طويلاً، وبسبب بنية الشبكة تصبح المعالجة بشكل تفرعي أمراً صعبا أو حتى مستحيل
%\textLR{\cite{TransformerCV}}.
%المشكلة الثانية هي تلاشي المشتقات
%\textLR{vanishing gradient}
%وهو ما يزيد من صعوبة التدريب وبخاصة في حالة السلاسل الطويلة. إحدى أشهر الحلول لتقليل أثر هذه المشكلة هي
%\textLR{LSTM}.
%من الممكن أن تختلف أبعاد سلسلتي الدخل والخرج ولحل هذا الاختلاف في الطول يمكن استخدام شبكتين عوديتين منفصلتين الأولى نسميها الـ
%\textLR{encoder}
%المرمز والثانية الـ
%\textLR{decoder}.
%\subsection{
%بنية المرمز
%\textLR{encoder}
%والـ
%\textLR{decoder}}
%يعالج المرمز كل عنصر من عناصر سلسلة الدخل ويستخرج المعلومات ويضعها في شعاع يسمى شعاع السياق
%\textLR{context vector}،
%بعد انتهاء المرمز من معالجة كامل عناصر الدخل يرسل شعاع السياق إلى الـ
%\textLR{decoder}
% ليقوم بتوليد سلسلة الخرج بشكل تتابعي كما في الشكل
%\ref{fig:RNN1}.
%\newline
%المشكلة الأساسية في هذه النماذج هي كيفية توليد شعاع السياق ليعبر بأفضل طريقة ممكنة عن معلومات سلسلة الدخل. فإذا كان شعاع السياق هو الحالة المخفية لآخر خطوة زمنية في سلسلة الدخل عندها لن يعبر بشكل فعال عن بدايات السلسلة وخصوصاً في حالة سلسلة الدخل الطويلة
%\textLR{\cite{IllustratedAttention}}.
%عندها يمكن أن يكون شعاع السياق عبارة عن كامل الحالات المخفية لكل خطوة زمنية في سلسلة الدخل، أو أن ينتج عن تعديل هذه الحالات المخفية بتطبيق تابع معين. ومن هنا ظهرت فكرة الانتباه 
%\textLR{attention}،
%وذلك بالتركيز فقط على الأجزاء المهمة وجعل الأجزاء غير المهمة أقل تأثيراً
%\textLR{\cite{TransformerCV}}.
%بما أن المشكلة الأساسية هي كيفية توليد شعاع السياق كان هناك الكثير من الأبحاث لتحسين تمثيل هذا الشعاع من أبرزها
%\textLR{\cite{Bahdanau2016},\cite{Luong2015}}.
%إذ كانت هذه الأعمال بدايات فكرة الانتباه، والذي أظهر نتائج جيدة ومبشرة في تطبيقات ترجمة الآلة، وخاصة في حالة الأبعاد الكبيرة لسلاسل الدخل
%\textLR{\cite{IllustratedAttention}}.
%
%\section{المحول \label{section:transformer}}
%ظهر نموذج المحول
%\textLR{Transformer\cite{Vaswani17}}
%في عام $2017$ ولأول مرة تم اختبار نموذج يعتمد بشكل كامل على توابع الانتباه ويستغني عن
%\textLR{RNN}،
% مما جنب نموذج المحول العديد من مشاكل هذا النوع من الشبكات والتي أهمها عدم القدرة على المعالجة التفرعية لعناصر الدخل، إذ أن أحد ميزات المحول هي قابلية معالجة العمليات الحسابية بشكل تفرعي كما سنرى لاحقاً وهذا ما يسرع عملية التدريب و الـ 
%\textLR{inference}
%في حال وجود
%\textLR{GPU}.
%\subsection{بنية المحول}
%\input{sympol_transformer_table.tex}          
%إن التطبيق الأساسي لنموذج المحول الأصلي
%\textLR{\cite{Vaswani17}}
%هو ترجمة الجمل من اللغة الإنكليزية إلى لغات أخرى، إذ أن دخل النموذج سلسلة من الكلمات (عدة جمل)، كل كلمة يتم التعبير عنها بشعاع من الأرقام ندعوه بالـ
%\textLR{token}،
%فيكون دخل المرمز هو عبارة عن مصفوفة أشعة
%$X \in \mathds{R}^{L \mathsf{x}d}$.
%\newline
%لتمييز مكان كل عنصر من السلسلة  يتم إضافة قيم للترميز المكاني
%\textLR{PE positional encoding\ref{PE}}.
%خرج كل طبقة من طبقات المرمز وطبقات الـ
%\textLR{decoder}
%هو مصفوفة أشعة بأبعاد
%$L \mathsf{x} d$.
%كما يوضح الشكل
%\ref{fig:Transformer}
%يتكون كل من المرمز والـ
%\textLR{decoder}
% من عدة كتل متسلسلة ومتطابقة في البنية.
%\subsection{المرمز}
%بداخل كل كتلة مرمز كتلتين أساسيتين كما في الشكل
%\ref{fig:enc_dec}
% الأولى لحساب الانتباه الذاتي متعدد الرؤوس 
%\textLR{MHA multi-head self attention\ref{att}},
%مهمة هذا التابع هو تعديل تمثيل عناصر سلسلة الدخل بحسب السياق.
%والكتلة الثانية هي شبكة عصبونية
%\textLR{FFN}
%تتكون من طبقتين مع تابع تفعيل 
%\textLR{ReLU}
%بحسب المعادلة
%\ref{eq:MLP}.
%\begin{equation}
%FFN(x) = max(0,xW_1+b_1)W_2+b_2
%\label{eq:MLP}
%\end{equation}
%تطبق هذه الشبكة على كل
%\textLR{token}
%أو عنصر دخل بشكل منفصل ومتطابق وهذا مانسميه
%\textLR{position-wise}
%كما في الشكل
%\ref{fig:positionWise}
%، مما يجعل المحول قادر على إجراء الحسابات بشكل تفرعي.
%من أجل كل من الكتلتين السابقتين يتم تطبيق
%\textLR{residual connection \cite{residual}}،
% متبوعة بـ
%\textLR{layer normalization\cite{LN}}.
%\newline
%\selectlanguage{english}
%$\text{LayerNorm}(x+\text{Sublayer}(x))$
%\selectlanguage{arabic}
%حيث
%\textLR{sublayer}
%هي تابع الانتباه أو الشبكة العصبونية الأمامية.
%\selectlanguage{english}
%\begin{figure}[H]
%	\centering
%	\includegraphics[scale=0.5]{images/Transformer}
%	\caption{\cite{Vaswani17}
%		\textRL{البنية الأساسية للمحول}}
%	\label{fig:Transformer}
%\end{figure}
%
%\begin{figure}[!h]
%	\centering
%	\includegraphics[scale=0.3]{images/encoder_decoder_illustrated_transformer}
%	\caption{\cite{illustratedTransformer}
%		\textRL{داخل المحول}
%		decoder
%		\textRL{البنية الأساسية للمرمز وللـ}}
%	\label{fig:enc_dec}
%\end{figure}
%
%\begin{figure}[!h]
%	\centering
%	\includegraphics[scale=0.3]{images/position_wise_illustrated_transformer}
%	\caption{\cite{illustratedTransformer}
%		\textRL{تطبيق التوابع داخل المحول لكل عنصر دخل بشكل تفرعي}}
%	\label{fig:positionWise}
%\end{figure}
%\selectlanguage{arabic}
% 
%\subsection{\textLR{decoder}}
%كما في الشكل
%\ref{fig:Transformer}،\ref{fig:enc_dec}
%كما في المرمز فإنه يتكون بالإضافة إلى كتلتي الانتباه الذاتي والشبكة الأمامية فإنه أيضاً يحتوي على كتلة إضافية وهي الانتباه التقاطعي متعدد الرؤوس 
%\ref{MCHA}
%، دخل هذا التابع هو دخل الـ
%\textLR{decoder}
% وخرج اخر طبقة في المرمز.
%\subsection{ترميز المعلومات المكانية
%\textLR{positional encoding\label{PE}}}
%كما قلنا سابقاً أنه قبل نموذج المحول
%\textLR{\cite{Vaswani17}}
%فإن معظم نماذج
%\textLR{seq2seq}
%كانت تستخدم الشبكات العودية
%\textLR{RNN}
%مع 
%\textLR{LSTM}،
%وهذا النوع من الشبكات يحافظ على المعلومات المكانية النسبية لعناصر سلسلة الدخل،  لكن الاستغناء عن شبكات 
%\textLR{RNN}
%والاستعانة فقط بتوابع الانتباه لمعالجة السلاسل يفقد المعلومات المكانية.
%\newline
%لمعالجة هذه المشكلة كان من اللازم إدخال المعلومات المكانية للسلسلة بشكل ما، هنا تم طرح طريقة ترميز الموقع لإضافة المعلومات المكانية لكل عنصر من عناصر السلسلة وذلك بإضافة قيم مستخرجة من توابع بترددات مختلفة كما في
%\textLR{\cite{Vaswani17}}.
%\begin{equation}
%\begin{split}
%	&PE_{(pos,2i)} = sin(pos/1000^{2i/d_{model}})\\
%	&PE_{(pos,2i+1)} = cos(pos/1000^{2i/d_{model}})\\
%\end{split}
%\end{equation}
%يتم إضافة هذه القيم إلى دخل كل من المرمز والـ
%\textLR{decoder}.
%\newline
%اختبرت العديد من الأبحاث طرق الترميز المكاني، فأبسط الأشكال هي إسناد رقم طبيعي أو حقيقي مميز أو عدد بين في المجال
%$[0,1]$
% إلى كل عنصر من عناصر السلسلة
%\textLR{\cite{web:PE}}.
%\newline
%لكن هذه الطرق البسيطة لم تحسن النتائج لأنها لم تستطع أن تجعل النموذج يلتقط معلومات المواقع بين العناصر.
%هنا اقترح البحث 
%\textLR{\cite{Vaswani17}}
%أن يكون ترميز الموقع تابع جيبي، إذ يمكن اعتبار  سلسلة الدخل سلسلة زمنية وكل عنصر هو خرج السلسلة عند خطوة زمنية معينة
%\textLR{\cite{web:PE}}،
%هذه الطريقة في التفكير ساعدت النموذج على كشف معلومات الموقع النسبية بين العناصر.
% 
%في الملاحق المستخدم في بحثنا
%\textLR{SwinTrack\cite{swinTrack}\ref{section:swintrack}}
%تم استخدام 
%\textLR{untied PE \cite{untiedPE}}
%مع بارامترات قابلة للتدريب.
%\input{attention.tex}
%\input{transformerCV.tex}
%\input{swinTransformer.tex}
%\input{transformer_in_tracking.tex}
%\section{خاتمة}
%تحدثنا في هذا الفصل عن نموذج المحول وكيفية تطوره باستخدامه لتوابع الانتباه فقط وبذلك تخلص من مشاكل الشبكات العودية، وقد تم شرح بنية المحول الأصلي مع التركيز على شرح تابع الانتباه والذي استخدمناه لدمج سمات الصور في نموذجنا كما سنتحدث في الفصل القادم، وذكرنا تطبيقات المحول في الرؤية الحاسوبية، مع شرح مفصل لنموذج
%\textLR{SwinTransformer}
%وبينا سبب استخدامنا له كون مناسب لتطبيقات الرؤية الصنعية إذ أنه أقل تعقيداً من نموذج المحول الأصلي في حال صورة دخل بأبعاد كبيرة.
%وفي الفقرة الأخيرة تحدثنا عن تطبيقات المحول في  الملاحقة والتي بدأت منذ $2021$ .


